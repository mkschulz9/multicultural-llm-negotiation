from typing import Optional
from openai import OpenAI


def prompt_llm_with_retry(
    messages: list[dict],
    openAI_client: OpenAI,
    max_attempts: int = 3,
    stream_content: bool = False,
) -> tuple[str, str, str]:
    """
    Attempts up to `max_attempts` times to get a valid response from the LLM. If both the user-facing response and the 'thinking' text are returned, the function succeeds. Otherwise, it retries until the limit is reached.

    Args:
        messages (list[dict]): A list of messages representing the conversation history, formatted for OpenAI Chat API.
        openAI_client (OpenAI): An instance of the OpenAI client used to send the request.
        max_attempts (int, optional): Maximum number of attempts to retry the LLM call on failure. Defaults to 3.
        stream_content (bool, optional): If True, streams the LLM response token-by-token. Defaults to False.

    Returns:
        returns (tuple[str, str, str]): A tuple containing:
        response (final user-facing response generated by the LLM), reasoning (internal reasoning or explanation generated by the LLM), user_query (original user query extracted or derived from the message context)
    """
    # Attempt to prompt LLM max_attempts times
    for i in range(1, max_attempts + 1):
        # Get response and reasoning from LLM
        response, reasoning = prompt_llm(
            messages=messages,
            openAI_client=openAI_client,
            stream_content=stream_content,
        )
        # Return response and reasoning if not empty
        if response:
            # Retrieves original query used to prompt LLM
            user_query = messages[-1]["content"]
            return (
                response,
                reasoning,
                user_query,
            )
        print(
            f"LLM response or reasoning empty, re-prompting (Attempt {i+1}/{max_attempts})"
        )

    raise RuntimeError(
        "LLM failed to generate a valid response after multiple attempts."
    )


def prompt_llm(
    messages: list[dict],
    openAI_client: OpenAI,
    model: str = "deepseek/r1-distill-llama-70b/fp-8",
    stream_content: bool = False,
) -> Optional[tuple[str, str]]:
    """
    Sends prompt to LLM using the OpenAI client, with optional streaming. Splits response into internal 'thinking' segment and a final user-facing response, based on the presence of a `</think>` token in the LLM output.

    Args:
        messages (list[dict]): A list of message dictionaries formatted for the OpenAI Chat API. The last message is modified to append a "<think>" token to guide the model.
        openAI_client (OpenAI): An instance of the OpenAI client used to make the chat completion request.
        model (str, optional): The model identifier to use for the request. Defaults to "deepseek/r1-distill-llama-70b/fp-8".
        stream_content (bool, optional): If True, streams the response token by token to stdout. Defaults to False.

    Returns:
        returns (Optional[tuple[str, str]]): A tuple containing: `final_response` (user-facing part of the LLM response) and `think_response` (internal reasoning/thinking portion generated before `</think>`)
            Returns (None, None) if an error occurs during request or response processing.
    """
    # Append a <think> token to signal model to think (necessary for specific default model)
    messages[-1]["content"] += " <think>"

    try:
        # Call OpenAI Chat API
        chatCompletion_response = openAI_client.chat.completions.create(
            model=model, messages=messages, stream=stream_content
        )
    except Exception as e:
        # Catch exception (if call fails) and return empty response and reasoning
        print(f"Error during call to OpenAI Chat API: {e}")
        return None, None

    # Response handling block (includes streaming handling)
    try:
        print("Trying to parse LLM response...\n")
        llm_thinking = False
        buffer = ""
        accumulated_response = ""
        think_open_tag = "<think>"
        think_close_tag = "</think>"

        # Iterate over the streamed response chunk objects
        for chunk in chatCompletion_response:
            # Get response content
            chunk_content = chunk.choices[0].delta.content
            if not chunk_content:
                continue
            else:
                accumulated_response += chunk_content

            # Check if LLM is thinking
            if chunk_content == think_open_tag:
                llm_thinking = True
                print("LLM thinking...")
                continue

            # If LLM is thinking, check if it has finished thinking
            if llm_thinking and chunk_content == think_close_tag:
                llm_thinking = False
                continue

            # Print the LLM response if it has finished thinking
            if not llm_thinking:
                buffer += chunk_content
                if len(buffer) > 20 or "\n" in buffer:
                    print(buffer.lstrip("\n"), end="", flush=True)
                    buffer = ""

        # Catch and print any unprinted buffer content
        if buffer:
            print(buffer.lstrip("\n"), end="", flush=True)

        print()
        # Parse accumulated response it LLM thought
        if (
            think_open_tag in accumulated_response
            and think_close_tag in accumulated_response
        ):
            start = accumulated_response.index(think_open_tag)
            end = accumulated_response.index(think_close_tag)

            # Reasoning is between the tags
            reasoning = accumulated_response[start + len(think_open_tag) : end]

            # Response is everything else
            response = (
                accumulated_response[:start]
                + accumulated_response[end + len(think_close_tag) :]
            ).lstrip("\n")
        else:
            # No explicit reasoning section
            response = accumulated_response.lstrip("\n")
            reasoning = ""

        return response, reasoning
    except Exception as e:
        print(f"Error during LLM response streaming: {e}\n{"~"*50}")
        return None, None
