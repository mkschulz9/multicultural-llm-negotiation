from typing import Optional
from openai import OpenAI


def prompt_llm_with_retry(
    messages: list[dict],
    openAI_client: OpenAI,
    max_attempts: int = 3,
    stream_content: bool = False,
) -> tuple[str, str, str]:
    """
    Attempts up to `max_attempts` times to get a valid response from the LLM. If both the user-facing response and the 'thinking' text are returned, the function succeeds. Otherwise, it retries until the limit is reached.

    Args:
        messages (list[dict]): A list of messages representing the conversation history, formatted for OpenAI Chat API.
        openAI_client (OpenAI): An instance of the OpenAI client used to send the request.
        max_attempts (int, optional): Maximum number of attempts to retry the LLM call on failure. Defaults to 3.
        stream_content (bool, optional): If True, streams the LLM response token-by-token. Defaults to False.

    Returns:
        returns (tuple[str, str, str]): A tuple containing:
        response (final user-facing response generated by the LLM), reasoning (internal reasoning or explanation generated by the LLM), user_query (original user query extracted or derived from the message context)
    """
    # Attempt to prompt LLM max_attempts times
    for i in range(1, max_attempts + 1):
        # Get response and reasoning from LLM
        response, reasoning = prompt_llm(
            messages=messages,
            openAI_client=openAI_client,
            stream_content=stream_content,
        )
        # Return response and reasoning if not empty
        if response and reasoning:
            # Retrieves original query used to prompt LLM
            user_query = messages[-1]["content"]
            return (
                response,
                reasoning,
                user_query,
            )
        print(
            f"LLM response or reasoning empty, re-prompting (Attempt {i+1}/{max_attempts})"
        )

    raise RuntimeError(
        "LLM failed to generate a valid response after multiple attempts."
    )


def prompt_llm(
    messages: list[dict],
    openAI_client: OpenAI,
    model: str = "deepseek/r1-distill-llama-70b/fp-8",
    stream_content: bool = False,
) -> Optional[tuple[str, str]]:
    """
    Sends prompt to LLM using the OpenAI client, with optional streaming. Splits response into internal 'thinking' segment and a final user-facing response, based on the presence of a `</think>` token in the LLM output.

    Args:
        messages (list[dict]): A list of message dictionaries formatted for the OpenAI Chat API. The last message is modified to append a "<think>" token to guide the model.
        openAI_client (OpenAI): An instance of the OpenAI client used to make the chat completion request.
        model (str, optional): The model identifier to use for the request. Defaults to "deepseek/r1-distill-llama-70b/fp-8".
        stream_content (bool, optional): If True, streams the response token by token to stdout. Defaults to False.

    Returns:
        returns (Optional[tuple[str, str]]): A tuple containing: `final_response` (user-facing part of the LLM response) and `think_response` (internal reasoning/thinking portion generated before `</think>`)
            Returns (None, None) if an error occurs during request or response processing.
    """
    # print("0" * 75)
    # print(messages)
    # print("0" * 75)
    # Append a <think> token to signal model to think (necessary for specific default model)
    messages[-1]["content"] += " <think>"

    try:
        # Call OpenAI Chat API
        chatCompletion_response = openAI_client.chat.completions.create(
            model=model, messages=messages, stream=stream_content
        )
    except Exception as e:
        # Catch exception (if call fails) and return empty response and reasoning
        print(f"Error during call to OpenAI Chat API: {e}")
        return None, None

    # Streaming response handling block
    try:
        # Iterate over chunks and stream if streaming
        if stream_content:
            accumulated_content = ""
            llm_thinking = True
            print(f"LLM Thinking...\n{"~"*50}")

            # Iterates over the streamed response chunk objects
            for chatCompletion_chunk in chatCompletion_response:
                # Get response content
                chunk_content = chatCompletion_chunk.choices[0].delta.content
                # If content is not empty
                if chunk_content:
                    accumulated_content += chunk_content
                    # Check for token, signaling LLM finished thinking
                    if chunk_content == "</think>":
                        llm_thinking = False
                        continue
                    # If LLM is not thinking, stream response
                    elif not llm_thinking:
                        print(
                            chunk_content.lstrip("\n"),
                            end="",
                            flush=True,
                        )
            print(f"\n{"~"*50}")
        # Just grab full content response if not streaming
        else:
            accumulated_content = chatCompletion_response.choices[0].message.content

        # Split content into thinking and non-thinking parts
        parts = accumulated_content.split("</think>", maxsplit=1)

        # Remove leading and trailing whitespaces and return content
        think_response = parts[0].strip()
        final_response = parts[1].strip()
        return final_response, think_response
    # Error during LLM content extraction (streaming or not)
    except Exception as e:
        print(f"Error in LLM response: {e}\n{"~"*50}")
        return None, None
